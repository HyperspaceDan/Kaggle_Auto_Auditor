{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyperspaceDan/Kaggle_Auto_Auditor/blob/main/Kaggle_Auto_Auditor_(CapstoneProject).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS7bjwmt5KEQ"
      },
      "source": [
        "<h1>1. Configuration:</h1>\n",
        "\n",
        "1.1 Installing and upgrading the necessary Python libraries.\n",
        "\n",
        "This step ensures that all the required libraries for the project are installed and up-to-date. We will be using:\n",
        "*   **`google-genai`**: The official Google Python SDK for interacting with the Gemini family of models.\n",
        "*   **`langgraph`**: A library for building stateful, multi-agent applications with LLMs, which we will use to define our workflow.\n",
        "*   **`langchain` and `langchain-google-genai`**: The core framework and its integration for using Google's generative AI models within a structured workflow.\n",
        "*   **`kaggle`**: The official Kaggle API client, which allows us to programmatically download notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGhKguDJQRzh"
      },
      "outputs": [],
      "source": [
        "!pip install -U google-genai langgraph langchain langchain-google-genai\n",
        "!pip install -q --upgrade kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlP5-Gea5u4z"
      },
      "source": [
        "1.2 Importing the libraries required for building the application.\n",
        "\n",
        "Here, we import all the necessary modules and classes that will be used throughout the notebook. This includes:\n",
        "*   Standard libraries like `os`, `re`, `json`, `subprocess`, `tempfile`, `shutil`, and `datetime` for system interactions, regular expressions, data handling, file operations, and timestamps.\n",
        "*   `requests` and `nbformat` for fetching and parsing notebook files.\n",
        "*   `google.genai` to configure the connection to the Google AI services.\n",
        "*   `langgraph` components (`StateGraph`, `END`) to construct the agentic workflow.\n",
        "*   `langchain_core` and `langchain_google_genai` for creating prompt templates, defining runnables (`RunnableWithFallbacks`, `RunnableConfig`), and initializing the LLM (`ChatGoogleGenerativeAI`).\n",
        "*   `pydantic` and `typing` for robust data structuring (`BaseModel`, `Field`) and type hinting (`TypedDict`, `Optional`, `List`, etc.), which helps in defining the shared state of our application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qYgt0sluTeQ0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.genai as genai\n",
        "import requests, nbformat, pydantic\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import re\n",
        "import tempfile\n",
        "import shutil\n",
        "import json\n",
        "import subprocess\n",
        "from typing import TypedDict, Tuple, Optional, Dict, List, Any\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnableWithFallbacks, RunnableConfig\n",
        "from pydantic import BaseModel, Field\n",
        "import datetime, time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daXuJsxz5-o8"
      },
      "source": [
        "1.3. Accessing the Gemini API key.\n",
        "\n",
        "To use Google's Gemini models, we need to authenticate our requests with an API key. This cell retrieves the `GEMINI_API_KEY` from the secure secret manager. It then sets this key as an environment variable (`os.environ['GOOGLE_API_KEY']`), which allows the Google client libraries to automatically find and use it for authenticating API calls.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kEYbdAAyTkbv"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWvACvIs61m0"
      },
      "source": [
        "1.4. Authenticating the Kaggle API.\n",
        "\n",
        "To download notebooks directly from Kaggle, we need to authenticate with the Kaggle API. This cell initializes the `KaggleApi` client and calls the `authenticate()` method. This method automatically looks for your `kaggle.json` credentials file to establish a secure connection. A success message will be printed upon successful authentication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "tifTTwMX43oP",
        "outputId": "c8180768-a8f0-4ab9-dd8d-f5eed7f697e4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e0e4fd84-cde9-4610-a9ab-33118e9253e8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e0e4fd84-cde9-4610-a9ab-33118e9253e8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "✅ Kaggle API Authenticated Successfully\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Ensure the .kaggle directory exists\n",
        "os.makedirs(os.path.expanduser('~/.config/kaggle'), exist_ok=True)\n",
        "\n",
        "if uploaded:\n",
        "    uploaded_filename = list(uploaded.keys())[0]\n",
        "    # Move the uploaded kaggle.json to the correct directory\n",
        "    !mv \"{uploaded_filename}\" ~/.config/kaggle/kaggle.json\n",
        "    # Set appropriate permissions\n",
        "    !chmod 600 ~/.config/kaggle/kaggle.json\n",
        "else:\n",
        "    print(\"❌ No kaggle.json file was uploaded. Please upload your kaggle.json file.\")\n",
        "    # Optionally exit or raise an error if no file was uploaded\n",
        "\n",
        "# Now import kaggle and authenticate\n",
        "import kaggle\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "print(\"✅ Kaggle API Authenticated Successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PJK5Feh727T"
      },
      "source": [
        "<h1>2. Defining the Workflow State and Tools</h1>\n",
        "\n",
        "<h3>2.1. Defining the Central 'Memory' Object (GraphState)</h3>\n",
        "\n",
        "We define the `GraphState` class, which serves as the shared memory for the entire multi-agent system. The state is passed between each node (agent) in the graph.\n",
        "\n",
        "Each agent can read data from the state (e.g., the notebook content) and write its output back to the state (e.g., a `code_report`). This ensures a seamless flow of information as the notebook progresses through the audit pipeline. The fields are marked as `Optional` because they are populated sequentially.\n",
        "\n",
        "This cell also defines a helper function, `append_log`, which adds a timestamped entry to the `logs` field in the state. This creates an audit trail of the workflow's execution, which is useful for debugging and tracking the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKmxdTudnE6S",
        "outputId": "804fc30f-aaf0-4b57-e363-b20ae35a0986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphState (our shared memory) is now defined.\n"
          ]
        }
      ],
      "source": [
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our multi-agent workflow.\n",
        "    This is the central \"memory\" object that gets passed between nodes.\n",
        "\n",
        "    Attributes:\n",
        "        target_url: The initial Kaggle notebook URL to be audited.\n",
        "        notebook_content: The raw text content (code + markdown) extracted from the notebook.\n",
        "        code_report: The analysis report from the CodeAnalysisAgent.\n",
        "        doc_report: The analysis report from the DocumentationAgent.\n",
        "        capability_report: The audit report from the GenCapabilityAuditAgent.\n",
        "        final_report: The final JSON synthesis of all reports.\n",
        "        error_message: A field to capture any errors that occur during the process.\n",
        "        initial_audit_score: The initial audit score from the first capability audit.\n",
        "        iteration: The current iteration number in a corrective loop.\n",
        "        logs: A list of dictionaries representing an audit trail of actions.\n",
        "    \"\"\"\n",
        "    target_url: str\n",
        "\n",
        "    # All other fields are 'Optional' because they are\n",
        "    # filled in sequentially by the agents.\n",
        "    notebook_content: Optional[str]\n",
        "    code_report: Optional[str]\n",
        "    doc_report: Optional[str]\n",
        "    capability_report: Optional[str]\n",
        "    final_report: Optional[str]\n",
        "    error_message: Optional[str]\n",
        "\n",
        "    # Fields for corrective loop logic and final report synthesis\n",
        "    initial_audit_score: Optional[int]\n",
        "    iteration: int\n",
        "    logs: Optional[List[Dict]] # Field for audit trail\n",
        "\n",
        "\n",
        "def append_log(state: GraphState, node_name: str, status: str, details: Dict) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Helper function to append an entry to the logs in the GraphState.\n",
        "\n",
        "    Args:\n",
        "        state: The current GraphState.\n",
        "        node_name: The name of the node generating the log.\n",
        "        status: The status of the operation (e.g., \"SUCCESS\", \"ERROR\", \"INFO\").\n",
        "        details: A dictionary containing additional details about the log entry.\n",
        "\n",
        "    Returns:\n",
        "        A new list containing the updated logs.\n",
        "    \"\"\"\n",
        "    current_logs = state.get('logs', [])\n",
        "\n",
        "    # Convert non-JSON serializable details to strings\n",
        "    serializable_details = {}\n",
        "    for k, v in details.items():\n",
        "        if isinstance(v, Exception):\n",
        "            serializable_details[k] = str(v)\n",
        "        else:\n",
        "            serializable_details[k] = v\n",
        "\n",
        "    new_log_entry = {\n",
        "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "        \"iteration\": state.get('iteration', 0),\n",
        "        \"node\": node_name,\n",
        "        \"status\": status,\n",
        "        \"details\": serializable_details\n",
        "    }\n",
        "    return current_logs + [new_log_entry]\n",
        "\n",
        "print(\"GraphState (our shared memory) is now defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlHQza0lwjC0"
      },
      "source": [
        "<h3>2.2. Defining the Kaggle Notebook Fetcher Tool</h3>\n",
        "\n",
        "This cell defines the `KaggleNotebookFetcher` class, a specialized tool responsible for handling all interactions with Kaggle notebooks. It has three main responsibilities:\n",
        "\n",
        "1.  **`parse_url`**: Takes a standard Kaggle notebook URL and uses regular expressions to extract the `owner_slug` and `kernel_slug`, which are needed for the API call.\n",
        "2.  **`fetch_kernel`**: Uses the `subprocess` module to run the Kaggle CLI command (`kaggle kernels pull`). This command downloads the notebook files into a temporary directory. The method then finds the `.ipynb` file and returns its path.\n",
        "3.  **`extract_content`**: Reads the `.ipynb` file (which is in JSON format), iterates through its cells, and separates the code and markdown content into two distinct strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "807e7279"
      },
      "outputs": [],
      "source": [
        "class KaggleNotebookFetcher:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Kaggle Notebook Fetcher and authenticates the Kaggle API client.\n",
        "        \"\"\"\n",
        "        self.api = KaggleApi()\n",
        "        self.api.authenticate()\n",
        "        print(\"KaggleApi client initialized and authenticated.\")\n",
        "\n",
        "    def parse_url(self, url: str) -> Tuple[str, str]:\n",
        "        \"\"\"\n",
        "        Parses a Kaggle notebook URL to extract the owner_slug and kernel_slug.\n",
        "\n",
        "        Args:\n",
        "            url: The Kaggle notebook URL (e.g., 'https://www.kaggle.com/code/{owner_slug}/{kernel_slug}').\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing (owner_slug, kernel_slug).\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the URL format is invalid.\n",
        "        \"\"\"\n",
        "        # Regex to match the typical Kaggle notebook URL format, including '/code/'\n",
        "        # Example: https://www.kaggle.com/code/owner_slug/kernel_slug\n",
        "        match = re.match(r'https://www\\.kaggle\\.com/code/([^/]+)/([^/]+)/?.*', url)\n",
        "        if match:\n",
        "            owner_slug = match.group(1)\n",
        "            kernel_slug = match.group(2)\n",
        "            return owner_slug, kernel_slug\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Invalid Kaggle notebook URL format. Expected 'https://www.kaggle.com/code/{{owner_slug}}/{{kernel_slug}}'. Got: {url}\"\n",
        "            )\n",
        "\n",
        "    def fetch_kernel(self, owner_slug: str, kernel_slug: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Downloads a Kaggle notebook kernel using the Kaggle CLI, extracts it, and finds the .ipynb file.\n",
        "\n",
        "        Args:\n",
        "            owner_slug: The owner's slug of the Kaggle notebook.\n",
        "            kernel_slug: The kernel's slug of the Kaggle notebook.\n",
        "\n",
        "        Returns:\n",
        "            The path to the extracted .ipynb file, or None if an error occurs.\n",
        "        \"\"\"\n",
        "        ipynb_file_path = None\n",
        "        # Use a temporary directory for the Kaggle CLI output\n",
        "        with tempfile.TemporaryDirectory() as base_temp_dir:\n",
        "            # Kaggle CLI will create its own subdirectory structure or place files directly\n",
        "            # We'll create a specific temporary directory for the pull command's output\n",
        "            kaggle_output_dir = os.path.join(base_temp_dir, f\"kaggle_notebook_{owner_slug}_{kernel_slug}\")\n",
        "            os.makedirs(kaggle_output_dir, exist_ok=True)\n",
        "\n",
        "            command = [\n",
        "                'kaggle',\n",
        "                'kernels',\n",
        "                'pull',\n",
        "                f'{owner_slug}/{kernel_slug}',\n",
        "                '-p',\n",
        "                kaggle_output_dir\n",
        "            ]\n",
        "            print(f\"Executing Kaggle CLI command: {' '.join(command)}\")\n",
        "\n",
        "            try:\n",
        "                # Run the kaggle kernels pull command\n",
        "                process = subprocess.run(\n",
        "                    command,\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    check=True  # Raises CalledProcessError for non-zero exit codes\n",
        "                )\n",
        "                print(\"Kaggle CLI stdout:\", process.stdout)\n",
        "                if process.stderr:\n",
        "                    print(\"Kaggle CLI stderr:\", process.stderr)\n",
        "\n",
        "                # Find the .ipynb file within the directory where Kaggle CLI pulled the kernel\n",
        "                # The CLI usually places the .ipynb file directly or within a simple subdirectory.\n",
        "                for root, _, files in os.walk(kaggle_output_dir):\n",
        "                    for file in files:\n",
        "                        if file.endswith('.ipynb'):\n",
        "                            ipynb_file_path = os.path.join(root, file)\n",
        "                            print(f\"Found .ipynb file: {ipynb_file_path}\")\n",
        "\n",
        "                            # Copy the .ipynb file to a new persistent temporary file\n",
        "                            # as base_temp_dir will be deleted after this block.\n",
        "                            final_ipynb_path = tempfile.NamedTemporaryFile(suffix='.ipynb', delete=False).name\n",
        "                            shutil.copy(ipynb_file_path, final_ipynb_path)\n",
        "                            print(f\"Copied .ipynb to persistent temp file: {final_ipynb_path}\")\n",
        "                            return final_ipynb_path\n",
        "\n",
        "                print(f\"No .ipynb file found in {kaggle_output_dir} after pulling {owner_slug}/{kernel_slug}.\")\n",
        "                return None\n",
        "\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"Kaggle CLI command failed with exit code {e.returncode}.\")\n",
        "                print(f\"Stderr: {e.stderr}\")\n",
        "                print(f\"Stdout: {e.stdout}\")\n",
        "                return None\n",
        "            except FileNotFoundError:\n",
        "                print(\"Error: 'kaggle' command not found. Ensure Kaggle CLI is installed and in PATH.\")\n",
        "                return None\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred during kernel pull: {e}\")\n",
        "                return None\n",
        "\n",
        "    def extract_content(self, notebook_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Loads an .ipynb file and extracts its code and markdown content, and raw cells.\n",
        "\n",
        "        Args:\n",
        "            notebook_path: The file path to the .ipynb notebook.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary with 'code', 'markdown' (concatenated content) and 'cells' (raw cell list).\n",
        "            Returns an empty dictionary with empty lists/strings if an error occurs.\n",
        "        \"\"\"\n",
        "        code_content = []\n",
        "        markdown_content = []\n",
        "        cells = []\n",
        "\n",
        "        try:\n",
        "            with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "                notebook_json = json.load(f)\n",
        "\n",
        "            cells = notebook_json.get('cells', [])\n",
        "\n",
        "            for cell in cells:\n",
        "                source = ''.join(cell.get('source', []))\n",
        "                if cell.get('cell_type') == 'code':\n",
        "                    code_content.append(source)\n",
        "                elif cell.get('cell_type') == 'markdown':\n",
        "                    markdown_content.append(source)\n",
        "\n",
        "            return {\n",
        "                'code': '\\n'.join(code_content),\n",
        "                'markdown': '\\n'.join(markdown_content),\n",
        "                'cells': cells\n",
        "            }\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Notebook file not found at {notebook_path}\")\n",
        "            return {'code': '', 'markdown': '', 'cells': []}\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: Could not decode JSON from notebook file at {notebook_path}\")\n",
        "            return {'code': '', 'markdown': '', 'cells': []}\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while extracting content from {notebook_path}: {e}\")\n",
        "            return {'code': '', 'markdown': '', 'cells': []}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLh0FWdlwjC1"
      },
      "source": [
        "<h1>3. Defining the Graph Nodes (Agents)</h1>\n",
        "\n",
        "<h3>3.1. Node 1: Ingest Notebook (Entry Point)</h3>\n",
        "\n",
        "Now we define the first node for our LangGraph workflow: `ingest_notebook`. This function serves as the entry point for our graph.\n",
        "\n",
        "It performs the following actions:\n",
        "1.  Initializes the `KaggleNotebookFetcher`.\n",
        "2.  Retrieves the `target_url` from the input state.\n",
        "3.  Calls the fetcher's methods to parse the URL, download the notebook, and extract its content.\n",
        "4.  Updates the `GraphState` with the `notebook_content` (a dictionary containing the code and markdown).\n",
        "5.  Handles any errors that occur during the process and records them in the state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "27aa6641"
      },
      "outputs": [],
      "source": [
        "def ingest_notebook(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    LangGraph node to ingest a Kaggle notebook from a given URL.\n",
        "    Fetches, parses, and extracts content, updating the GraphState.\n",
        "    \"\"\"\n",
        "    print(\"---INGESTING KAGGLE NOTEBOOK---\")\n",
        "    fetcher = KaggleNotebookFetcher()\n",
        "    target_url = state.get('target_url')\n",
        "\n",
        "    if not target_url:\n",
        "        error_msg = \"target_url not found in GraphState.\"\n",
        "        state['error_message'] = error_msg\n",
        "        state['logs'] = append_log(state, \"ingest_notebook\", \"ERROR\", {\"error\": error_msg})\n",
        "        return state\n",
        "\n",
        "    try:\n",
        "        # 1. Parse the URL\n",
        "        owner_slug, kernel_slug = fetcher.parse_url(target_url)\n",
        "        print(f\"Parsed URL: Owner='{owner_slug}', Kernel='{kernel_slug}'\")\n",
        "\n",
        "        # 2. Fetch the kernel\n",
        "        notebook_path = fetcher.fetch_kernel(owner_slug, kernel_slug)\n",
        "        if notebook_path is None:\n",
        "            error_msg = f\"Failed to fetch notebook for {owner_slug}/{kernel_slug}.\"\n",
        "            state['error_message'] = error_msg\n",
        "            state['logs'] = append_log(state, \"ingest_notebook\", \"ERROR\", {\"target_url\": target_url, \"owner_slug\": owner_slug, \"kernel_slug\": kernel_slug, \"error\": error_msg})\n",
        "            return state\n",
        "\n",
        "        # 3. Extract content\n",
        "        content = fetcher.extract_content(notebook_path)\n",
        "        state['notebook_content'] = content\n",
        "        print(f\"Successfully extracted content from {kernel_slug}.\")\n",
        "\n",
        "        # 4. Clean up the temporary .ipynb file\n",
        "        if os.path.exists(notebook_path):\n",
        "            os.remove(notebook_path)\n",
        "            print(f\"Cleaned up temporary file: {notebook_path}\")\n",
        "\n",
        "        state['logs'] = append_log(state, \"ingest_notebook\", \"SUCCESS\", {\"target_url\": target_url, \"owner_slug\": owner_slug, \"kernel_slug\": kernel_slug})\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"ValueError during ingestion: {e}\")\n",
        "        error_msg = f\"Invalid URL or parsing error: {e}\"\n",
        "        state['error_message'] = error_msg\n",
        "        state['logs'] = append_log(state, \"ingest_notebook\", \"ERROR\", {\"target_url\": target_url, \"error\": str(e)})\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during ingestion: {e}\")\n",
        "        error_msg = f\"Error during ingestion: {e}\"\n",
        "        state['error_message'] = error_msg\n",
        "        state['logs'] = append_log(state, \"ingest_notebook\", \"ERROR\", {\"target_url\": target_url, \"error\": str(e)})\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXPCVtSwjC2"
      },
      "source": [
        "<h3>3.2. Configuring the LLMs with Fallbacks</h3>\n",
        "\n",
        "Here, we configure the generative models that will serve as the \"brains\" for our agents. To enhance the reliability of our system, we implement a fallback mechanism.\n",
        "\n",
        "1.  **`llm_primary`**: The main `gemini-2.5-flash` model we intend to use.\n",
        "2.  **`llm_fallback`**: A second instance of the same model that will be used if the primary one fails.\n",
        "3.  **`llm_with_fallbacks`**: A new, resilient LangChain runnable object is created using `.with_fallbacks()`. If a call to `llm_primary` fails (e.g., due to a temporary API error), it will automatically retry the call with `llm_fallback`. This makes our application more robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20b9893a",
        "outputId": "81f7bc19-c2da-4718-bf26-fea671fdd870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGoogleGenerativeAI models with fallbacks initialized.\n"
          ]
        }
      ],
      "source": [
        "# 1. Instantiate a primary ChatGoogleGenerativeAI model\n",
        "llm_primary = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    convert_system_message_to_human=True\n",
        ")\n",
        "\n",
        "# 2. Instantiate a fallback ChatGoogleGenerativeAI model\n",
        "llm_fallback = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    convert_system_message_to_human=True\n",
        ")\n",
        "\n",
        "# 3. Create the llm_with_fallbacks object\n",
        "llm_with_fallbacks = llm_primary.with_fallbacks([llm_fallback])\n",
        "\n",
        "print(\"ChatGoogleGenerativeAI models with fallbacks initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnJhNe24wjC2"
      },
      "source": [
        "<h3>3.3. Node 2: Analyze Code (Code Reviewer Agent)</h3>\n",
        "\n",
        "This cell defines the `analyze_code` node, which acts as our **Code Reviewer Agent**.\n",
        "\n",
        "Its role is to:\n",
        "1.  Read the `notebook_content` from the shared `GraphState`.\n",
        "2.  Define a system prompt that gives the LLM a specific persona: a \"Senior Google Staff Software Engineer.\"\n",
        "3.  Instruct the LLM to review the code for reproducibility, efficiency, bugs, and best practices.\n",
        "4.  Invoke the `llm_with_fallbacks` chain with the code.\n",
        "5.  Save the generated analysis as a string into the `code_report` field of our `GraphState`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8d42cb0d"
      },
      "outputs": [],
      "source": [
        "def analyze_code(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    LangGraph node to analyze the code content of a Kaggle notebook.\n",
        "    Uses an LLM to provide a report on reproducibility, efficiency, and bugs.\n",
        "    \"\"\"\n",
        "    print(\"---ANALYZING CODE--- \")\n",
        "\n",
        "    notebook_content = state.get('notebook_content')\n",
        "    if not notebook_content or not notebook_content.get('code'):\n",
        "        error_msg = \"No code content found for analysis.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"analyze_code\", \"ERROR\", {\"error\": error_msg})\n",
        "        return state\n",
        "\n",
        "    code_to_analyze = notebook_content['code']\n",
        "\n",
        "    try:\n",
        "        # 1. Define the system prompt (persona and instructions)\n",
        "        system_prompt = (\n",
        "            \"You are a Senior Google Staff Software Engineer. Your task is to provide a comprehensive \"\n",
        "            \"code review of the given Python code from a Kaggle notebook. Focus on the following aspects: \"\n",
        "            \"reproducibility, efficiency, potential bugs, security vulnerabilities (if any), and adherence to best practices. \"\n",
        "            \"Provide constructive and detailed feedback in a clear, structured report format. \"\n",
        "            \"Start your response directly with the analysis report, without pleasantries or introductory phrases. \"\n",
        "            \"Your response should be a string of the analysis report.\"\n",
        "        )\n",
        "\n",
        "        # 2. Define the human message (including the code to analyze)\n",
        "        human_message = \"Analyze the following Python code:\\n\\n```python\\n{code}\\n```\"\n",
        "\n",
        "        # 3. Create a prompt template\n",
        "        prompt_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", human_message)\n",
        "        ])\n",
        "\n",
        "        # 4. Invoke the LLM with fallbacks\n",
        "        # The llm_with_fallbacks object was defined in the previous step.\n",
        "        full_chain = prompt_template | llm_with_fallbacks\n",
        "        response = full_chain.invoke({\"code\": code_to_analyze})\n",
        "\n",
        "        # Store the string output from the LLM's response\n",
        "        state['code_report'] = response.content\n",
        "        print(\"✅ Code analysis report generated successfully.\")\n",
        "\n",
        "        state['logs'] = append_log(state, \"analyze_code\", \"SUCCESS\", {\"report_snippet\": state['code_report'][:100] + \"...\" if state['code_report'] else \"\"})\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error during code analysis: {e}\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"analyze_code\", \"ERROR\", {\"error\": str(e)})\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EdKcxSKwjC3"
      },
      "source": [
        "<h3>3.4. Node 3: Analyze Documentation (Technical Writer Agent)</h3>\n",
        "\n",
        "Next, we define the `analyze_documentation` node, which acts as our **Technical Writer Agent**.\n",
        "\n",
        "This agent's responsibilities are:\n",
        "1.  Read the markdown content and the `code_report` from the `GraphState`.\n",
        "2.  Assume the persona of a \"Senior Technical Writer & Data Storyteller.\"\n",
        "3.  Analyze the notebook's documentation for clarity, narrative flow, and how well it explains the accompanying code. The `code_report` is provided as context to ensure the documentation review is relevant.\n",
        "4.  Save the generated documentation review into the`doc_report` field of the `GraphState`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "808ec8e8"
      },
      "outputs": [],
      "source": [
        "def analyze_documentation(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    LangGraph node to analyze the markdown (documentation) content of a Kaggle notebook.\n",
        "    Uses an LLM to provide a report on clarity, narrative flow, and explanation of code logic.\n",
        "    \"\"\"\n",
        "    print(\"---ANALYZING DOCUMENTATION---\")\n",
        "\n",
        "    notebook_content = state.get('notebook_content')\n",
        "    code_report = state.get('code_report') # Get the code report for context\n",
        "\n",
        "    if not notebook_content or not notebook_content.get('markdown'):\n",
        "        error_msg = \"No markdown content found for analysis.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"analyze_documentation\", \"ERROR\", {\"error\": error_msg})\n",
        "        return state\n",
        "\n",
        "    if not code_report:\n",
        "        error_msg = \"Code report not found. Documentation analysis requires code context.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"analyze_documentation\", \"ERROR\", {\"error\": error_msg})\n",
        "        return state\n",
        "\n",
        "    markdown_to_analyze = notebook_content['markdown']\n",
        "\n",
        "    try:\n",
        "        # 1. Define the system prompt (persona and instructions)\n",
        "        system_prompt = (\n",
        "            \"You are a Senior Technical Writer & Data Storyteller. Your task is to provide a comprehensive \"\n",
        "            \"review of the provided markdown documentation from a Kaggle notebook. \"\n",
        "            \"Analyze it for clarity, narrative flow, and most importantly, how well it explains and corresponds to the accompanying code logic. \"\n",
        "            \"Consider the code analysis report provided as context. \"\n",
        "            \"Provide constructive and detailed feedback in a clear, structured report format. \"\n",
        "            \"Start your response directly with the analysis report, without pleasantries or introductory phrases. \"\n",
        "            \"Your response should be a string of the analysis report.\"\n",
        "        )\n",
        "\n",
        "        # 2. Define the human message (including markdown and code report for context)\n",
        "        human_message = (\n",
        "            \"Analyze the following markdown documentation in the context of the provided code analysis report.\\n\\n\"\n",
        "            \"--- Markdown Documentation ---\\n\"\n",
        "            \"```markdown\\n{markdown}\\n```\\n\\n\"\n",
        "            \"--- Code Analysis Report (for context) ---\\n\"\n",
        "            \"```text\\n{code_report}\\n```\\n\"\n",
        "        )\n",
        "\n",
        "        # 3. Create a prompt template\n",
        "        prompt_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", human_message)\n",
        "        ])\n",
        "\n",
        "        # 4. Invoke the LLM with fallbacks\n",
        "        full_chain = prompt_template | llm_with_fallbacks\n",
        "        response = full_chain.invoke({\n",
        "            \"markdown\": markdown_to_analyze,\n",
        "            \"code_report\": code_report\n",
        "        })\n",
        "\n",
        "        # Store the string output from the LLM's response\n",
        "        state['doc_report'] = response.content\n",
        "        print(\"✅ Documentation analysis report generated successfully.\")\n",
        "\n",
        "        state['logs'] = append_log(state, \"analyze_documentation\", \"SUCCESS\", {\"report_snippet\": state['doc_report'][:100] + \"...\" if state['doc_report'] else \"\"})\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error during documentation analysis: {e}\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"analyze_documentation\", \"ERROR\", {\"error\": str(e)})\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUr6KKTswjC4"
      },
      "source": [
        "<h3>3.5. Node 4: Audit Capabilities (Principal Auditor Agent)</h3>\n",
        "\n",
        "This cell defines the `audit_capabilities` node, which acts as our **Principal Auditor Agent**.\n",
        "\n",
        "This agent performs a higher-level synthesis of the previous analyses:\n",
        "1.  It takes both the `code_report` and `doc_report` from the `GraphState`.\n",
        "2.  Assuming the persona of a \"Principal AI Researcher / Peer Reviewer,\" it prompts the LLM to synthesize these reports.\n",
        "3.  The LLM is instructed to generate a comprehensive audit, which must include:\n",
        "    *   A **Trust Score** (an integer from 0 to 100).\n",
        "    *   A list of the top 3 critical flaws.\n",
        "    *   An assessment of \"Correction Feasibility\" for each flaw.\n",
        "4.  The output is saved as the `capability_report`, and the initial trust score is recorded for later comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7288e2fd"
      },
      "outputs": [],
      "source": [
        "def audit_capabilities(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    LangGraph node to audit the overall capabilities of the notebook\n",
        "    by synthesizing code and documentation reports.\n",
        "    Assigns a Trust Score, identifies critical flaws, and assesses correction feasibility.\n",
        "    \"\"\"\n",
        "    print(\"---AUDITING CAPABILITIES---\")\n",
        "\n",
        "    code_report = state.get('code_report')\n",
        "    doc_report = state.get('doc_report')\n",
        "    current_iteration = state.get('iteration', 0)\n",
        "\n",
        "    if not code_report:\n",
        "        error_msg = \"Code report not found for capability audit.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"audit_capabilities\", \"ERROR\", {\"error\": error_msg, \"iteration\": current_iteration})\n",
        "        return state\n",
        "\n",
        "    if not doc_report:\n",
        "        error_msg = \"Documentation report not found for capability audit.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"audit_capabilities\", \"ERROR\", {\"error\": error_msg, \"iteration\": current_iteration})\n",
        "        return state\n",
        "\n",
        "    try:\n",
        "        # 1. Define the system prompt (persona and instructions)\n",
        "        system_prompt = (\n",
        "            \"You are a Principal AI Researcher / Peer Reviewer. Your task is to critically analyze \"\n",
        "            \"the provided 'Code Analysis Report' and 'Documentation Analysis Report' for a Kaggle notebook. \"\n",
        "            \"Synthesize these two reports to provide an overall audit, focusing on the notebook's \"\n",
        "            \"reliability, correctness, and clarity. Your output must include: \"\n",
        "            \"1. A 'Trust Score' (an integer from 0 to 100, where 100 is excellent).\"\n",
        "            \"2. Top 3 critical flaws identified across both code and documentation, prioritizing impact.\"\n",
        "            \"3. For each of the top 3 flaws, a brief assessment of 'Correction Feasibility' (e.g., 'Easy', 'Medium', 'Hard').\"\n",
        "            \"Provide constructive and detailed feedback in a clear, structured report format. \"\n",
        "            \"Start your response directly with the analysis report, without pleasantries or introductory phrases. \"\n",
        "            \"Your response should be a string of the audit report.\"\n",
        "        )\n",
        "\n",
        "        # 2. Define the human message\n",
        "        human_message = (\n",
        "            \"Please synthesize the following code and documentation analysis reports:\\n\\n\"\n",
        "            \"--- Code Analysis Report ---\\n\"\n",
        "            \"```text\\n{code_report}\\n```\\n\\n\"\n",
        "            \"--- Documentation Analysis Report ---\\n\"\n",
        "            \"```text\\n{doc_report}\\n```\\n\"\n",
        "        )\n",
        "\n",
        "        # 3. Create a prompt template\n",
        "        prompt_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", human_message)\n",
        "        ])\n",
        "\n",
        "        # 4. Invoke the LLM with fallbacks\n",
        "        full_chain = prompt_template | llm_with_fallbacks\n",
        "        response = full_chain.invoke({\n",
        "            \"code_report\": code_report,\n",
        "            \"doc_report\": doc_report\n",
        "        })\n",
        "\n",
        "        # Store the string output from the LLM's response\n",
        "        state['capability_report'] = response.content\n",
        "        print(\"✅ Capability audit report generated successfully.\")\n",
        "\n",
        "        # Extract the current trust score from the newly generated capability_report\n",
        "        current_trust_score = None\n",
        "        score_match = re.search(r'Trust Score: (\\d+)', state['capability_report'])\n",
        "        if score_match:\n",
        "            current_trust_score = int(score_match.group(1))\n",
        "\n",
        "        # Set initial_audit_score if it's the first time this node is run in a sequence (i.e., it's None in the incoming state)\n",
        "        if state.get('initial_audit_score') is None and current_trust_score is not None:\n",
        "            state['initial_audit_score'] = current_trust_score\n",
        "            print(f\"Initial Audit Score set to: {current_trust_score}\")\n",
        "\n",
        "        # Ensure iteration is explicitly carried forward in the state returned by audit_capabilities\n",
        "        state['iteration'] = current_iteration\n",
        "\n",
        "        state['logs'] = append_log(state, \"audit_capabilities\", \"SUCCESS\", {\"current_trust_score\": current_trust_score, \"iteration\": current_iteration, \"report_snippet\": state['capability_report'][:100] + \"...\" if state['capability_report'] else \"\"})\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error during capability audit: {e}\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        # Even in error, try to preserve iteration if possible for debugging loops\n",
        "        state['iteration'] = current_iteration\n",
        "        state['logs'] = append_log(state, \"audit_capabilities\", \"ERROR\", {\"error\": str(e), \"iteration\": current_iteration})\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64IdHvpRwjC4"
      },
      "source": [
        "<h3>3.6. Node 5: Generate Corrections (Mentor & Corrector Agent)</h3>\n",
        "\n",
        "This cell defines the `generate_corrections` node, which acts as our **Mentor & Corrector Agent**. This node is only activated if the audit score is below a certain threshold.\n",
        "\n",
        "Its purpose is to:\n",
        "1.  Take the `code_report`, `doc_report`, and the current `notebook_content` (specifically the list of cells) from the `GraphState`.\n",
        "2.  Assume the persona of a \"Senior Staff Engineer & Mentor.\"\n",
        "3.  Instruct the LLM to generate corrected versions of the notebook's cells based on the identified flaws.\n",
        "4.  The LLM is specifically prompted to return its output in a clean JSON format with a `cells` key, containing a list of cell objects.\n",
        "5.  This new, corrected list of cells overwrites the existing cells in `notebook_content`. The flattened `code` and `markdown` keys are then re-synchronized from this new list of cells, preparing the state for a re-evaluation loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "056ddf46"
      },
      "outputs": [],
      "source": [
        "from google.api_core.exceptions import ResourceExhausted\n",
        "\n",
        "def generate_corrections(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    LangGraph node to generate corrected code and markdown based on audit reports.\n",
        "    Uses an LLM to review reports and suggest improvements, preserving cell structure.\n",
        "    \"\"\"\n",
        "    print(\"--GENERATING CORRECTIONS--\")\n",
        "\n",
        "    code_report = state.get('code_report')\n",
        "    doc_report = state.get('doc_report')\n",
        "    notebook_content = state.get('notebook_content')\n",
        "    current_iteration = state.get('iteration', 0)\n",
        "\n",
        "    if not code_report and not doc_report:\n",
        "        error_msg = \"No code or documentation reports found to generate corrections.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"generate_corrections\", \"ERROR\", {\"error\": error_msg, \"iteration\": current_iteration})\n",
        "        return state\n",
        "\n",
        "    # Critical: Ensure 'cells' are present in notebook_content\n",
        "    if not notebook_content or not notebook_content.get('cells'):\n",
        "        error_msg = \"Current notebook cells not found in notebook_content for correction.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"generate_corrections\", \"ERROR\", {\"error\": error_msg, \"iteration\": current_iteration})\n",
        "        return state\n",
        "\n",
        "    llm_output_content = None # Initialize outside try block for wider scope\n",
        "\n",
        "    try:\n",
        "        # Get original cells to pass to the LLM for context\n",
        "        original_cells = notebook_content['cells']\n",
        "        original_cells_json = json.dumps(original_cells, indent=2) # Convert to JSON string for prompt\n",
        "\n",
        "        # 1. Define the system prompt (persona and instructions)\n",
        "        system_prompt = (\n",
        "            \"You are a Senior Staff Engineer & Mentor. Your task is to review the provided 'Code Analysis Report' and 'Documentation Analysis Report' for a Kaggle notebook. \"\n",
        "            \"Based on these reports, you must generate improved versions of the original notebook's cells. Focus on addressing the identified flaws, \"\n",
        "            \"improving clarity, reproducibility, efficiency, and narrative flow. \"\n",
        "            \"Your output MUST be a JSON object with a single key: 'cells'. The 'cells' key must contain a list of cell objects. \"\n",
        "            \"Each cell object MUST have two keys: 'cell_type' (either 'code' or 'markdown') and 'source' (the content of the cell as a string). \"\n",
        "            \"The 'source' for code cells should be Python code, and for markdown cells should be Markdown text. \"\n",
        "            \"Preserve the original interleaving of code and markdown cells as much as possible, only modifying the 'source' content as needed. \"\n",
        "            \"Do not include any other text outside the JSON. Ensure the JSON is perfectly formatted and valid.\"\n",
        "        )\n",
        "\n",
        "        # 2. Define the human message (including all relevant context)\n",
        "        human_message = (\n",
        "            \"Please provide corrected versions of the following Kaggle notebook's cells based on the analysis reports.\\n\\n\"\n",
        "            \"--- Code Analysis Report ---\\n\"\n",
        "            \"```text\\n{code_report}\\n```\\n\\n\"\n",
        "            \"--- Documentation Analysis Report ---\\n\"\n",
        "            \"```text\\n{doc_report}\\n```\\n\\n\"\n",
        "            \"--- Original Notebook Cells (for structure and context) ---\\n\"\n",
        "            \"```json\\n{original_cells_json}\\n```\\n\\n\"\n",
        "            \"Your response should be a JSON object with a single key 'cells' containing an array of cell objects, each with 'cell_type' and 'source'.\"\n",
        "        )\n",
        "\n",
        "        # 3. Create a prompt template\n",
        "        prompt_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", human_message)\n",
        "        ])\n",
        "\n",
        "        # 4. Invoke the LLM with fallbacks and rate limit handling\n",
        "        full_chain = prompt_template | llm_with_fallbacks\n",
        "        try:\n",
        "            response = full_chain.invoke({\n",
        "                \"code_report\": code_report,\n",
        "                \"doc_report\": doc_report,\n",
        "                \"original_cells_json\": original_cells_json\n",
        "            })\n",
        "        except ResourceExhausted:\n",
        "            print(\"⚠️ RATE LIMIT HIT. Sleeping for 60 seconds to reset quota...\")\n",
        "            time.sleep(60) # Force a cool-down\n",
        "            # Retry once after sleep\n",
        "            response = full_chain.invoke({\n",
        "                \"code_report\": code_report,\n",
        "                \"doc_report\": doc_report,\n",
        "                \"original_cells_json\": original_cells_json\n",
        "            })\n",
        "\n",
        "        # 5. Parse the LLM's response\n",
        "        llm_output_content = response.content\n",
        "\n",
        "        # Robust Regex Extraction\n",
        "        # This finds the first opening brace '{' and the last closing brace '}'\n",
        "        match = re.search(r'\\{.*\\}', llm_output_content, re.DOTALL)\n",
        "\n",
        "        if match:\n",
        "            json_str = match.group(0)\n",
        "            try:\n",
        "                corrections_json = json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                # Fallback: Try to clean common Markdown issues if strict parse fails\n",
        "                clean_str = json_str.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "                corrections_json = json.loads(clean_str)\n",
        "        else:\n",
        "            raise ValueError(\"No JSON object found in LLM response.\")\n",
        "\n",
        "\n",
        "        if 'cells' not in corrections_json or not isinstance(corrections_json['cells'], list):\n",
        "            raise ValueError(\"LLM response missing 'cells' key or 'cells' is not a list.\")\n",
        "\n",
        "        new_cells = corrections_json['cells']\n",
        "\n",
        "        # Validate each cell object structure\n",
        "        for cell in new_cells:\n",
        "            if not isinstance(cell, dict) or 'cell_type' not in cell or 'source' not in cell:\n",
        "                raise ValueError(\"Invalid cell object structure in LLM response.\")\n",
        "            if cell['cell_type'] not in ['code', 'markdown']:\n",
        "                raise ValueError(f\"Invalid cell_type '{cell['cell_type']}' in LLM response.\")\n",
        "\n",
        "        # 6. Update state with new cells\n",
        "        state['notebook_content']['cells'] = new_cells\n",
        "\n",
        "        # 7. CRITICAL SYNC STEP: Re-generate flattened code and markdown content from the new cells\n",
        "        new_code_content_list = []\n",
        "        new_markdown_content_list = []\n",
        "        for cell in new_cells:\n",
        "            if cell.get('cell_type') == 'code':\n",
        "                new_code_content_list.append(cell.get('source', ''))\n",
        "            elif cell.get('cell_type') == 'markdown':\n",
        "                new_markdown_content_list.append(cell.get('source', ''))\n",
        "\n",
        "        state['notebook_content']['code'] = '\\n'.join(new_code_content_list)\n",
        "        state['notebook_content']['markdown'] = '\\n'.join(new_markdown_content_list)\n",
        "\n",
        "        # 8. Increment iteration\n",
        "        state['iteration'] = current_iteration + 1\n",
        "\n",
        "        print(f\"✅ Corrections generated successfully. Iteration: {state['iteration']}\")\n",
        "        state['logs'] = append_log(state, \"generate_corrections\", \"SUCCESS\", {\"iteration\": state['iteration']})\n",
        "\n",
        "    except ValueError as e:\n",
        "        error_msg = f\"LLM output JSON format invalid: {e}. Output: '{llm_output_content[:500]}...'\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['iteration'] = current_iteration + 1 # Increment iteration to prevent infinite loops on repeated errors\n",
        "        state['logs'] = append_log(state, \"generate_corrections\", \"ERROR\", {\"error\": str(e), \"llm_output_prefix\": llm_output_content[:500], \"iteration\": state['iteration']})\n",
        "        return state\n",
        "    except Exception as e:\n",
        "        error_msg = f\"An unexpected error occurred during correction generation: {e}\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        # Increment iteration to prevent infinite loops on repeated errors\n",
        "        state['iteration'] = current_iteration + 1\n",
        "        state['logs'] = append_log(state, \"generate_corrections\", \"ERROR\", {\"error\": str(e), \"iteration\": state['iteration']})\n",
        "\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoPVm7KAwjC5"
      },
      "source": [
        "<h3>3.7. Node 6: Synthesize Report (Final Auditor Agent)</h3>\n",
        "\n",
        "This cell defines the `synthesize_report` node, which is the final step in our workflow. It acts as the **Final Auditor Agent**.\n",
        "\n",
        "Its responsibilities are:\n",
        "1.  Define a `FinalReport` Pydantic model to specify the exact JSON structure of the final output. This ensures a reliable, machine-readable result.\n",
        "2.  Take the `initial_audit_score` and the final `capability_report` from the `GraphState`.\n",
        "3.  Prompt the LLM to synthesize all the information into a final summary that conforms to the `FinalReport` schema.\n",
        "4.  The structured output from the LLM is then saved as a JSON string in the `final_report` field of the `GraphState`, concluding the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b88e5821",
        "outputId": "8597d2ce-df7e-4370-f22b-dae8f94034ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "synthesize_report function (LangGraph node) is defined, along with FinalReport Pydantic model.\n"
          ]
        }
      ],
      "source": [
        "class FinalReport(BaseModel):\n",
        "    initial_score: int = Field(..., description=\"The initial audit score of the notebook.\")\n",
        "    final_score: int = Field(..., description=\"The final audit score of the notebook after corrections.\")\n",
        "    improvements: List[str] = Field(..., description=\"A list of key improvements made during the correction process.\")\n",
        "    overall_summary: str = Field(..., description=\"An overall summary of the notebook's quality and the audit process.\")\n",
        "\n",
        "def synthesize_report(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    LangGraph node to synthesize a final JSON report based on all analysis.\n",
        "    Uses an LLM to generate a structured output according to the FinalReport Pydantic model.\n",
        "    \"\"\"\n",
        "    print(\"---SYNTHESIZING FINAL REPORT---\")\n",
        "\n",
        "    initial_audit_score = state.get('initial_audit_score')\n",
        "    capability_report = state.get('capability_report')\n",
        "    current_iteration = state.get('iteration', 0)\n",
        "\n",
        "    # Extract final_score from capability_report if available\n",
        "    final_score = None\n",
        "    if capability_report:\n",
        "        score_match = re.search(r'Trust Score: (\\d+)', capability_report)\n",
        "        if score_match:\n",
        "            final_score = int(score_match.group(1))\n",
        "\n",
        "    if initial_audit_score is None:\n",
        "        error_msg = \"Initial audit score not found for final report synthesis.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"synthesize_report\", \"ERROR\", {\"error\": error_msg, \"iteration\": current_iteration})\n",
        "        return state\n",
        "\n",
        "    if capability_report is None:\n",
        "        error_msg = \"Capability report not found for final report synthesis.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"synthesize_report\", \"ERROR\", {\"error\": error_msg, \"iteration\": current_iteration})\n",
        "        return state\n",
        "\n",
        "    if final_score is None:\n",
        "        error_msg = \"Could not extract final score from capability report.\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"synthesize_report\", \"ERROR\", {\"error\": error_msg, \"iteration\": current_iteration})\n",
        "        return state\n",
        "\n",
        "    try:\n",
        "        # 1. Define the system prompt (persona and instructions)\n",
        "        system_prompt = (\n",
        "            \"You are a Principal AI Researcher / Final Auditor. Your task is to synthesize all available information \"\n",
        "            \"about a Kaggle notebook's audit process into a final, comprehensive report. \"\n",
        "            \"The report should clearly summarize the notebook's quality, the improvements made, and the overall outcome of the audit. \"\n",
        "            \"Your output MUST be a JSON object that strictly adheres to the provided Pydantic schema for `FinalReport`. \"\n",
        "            \"Include the initial score, the final score, a list of key improvements (if any), and an overall summary. \"\n",
        "            \"Do not include any other text outside the JSON. Be concise and factual.\"\n",
        "        )\n",
        "\n",
        "        # 2. Define the human message\n",
        "        human_message = (\n",
        "            \"Please generate a final audit report based on the following information:\\n\\n\" +\n",
        "            f\"Initial Audit Score: {initial_audit_score}\\n\" +\n",
        "            f\"Current Iteration: {current_iteration}\\n\" +\n",
        "            \"--- Latest Capability Report ---\\n\" +\n",
        "            \"```text\\n{capability_report}\\n```\\n\\n\" +\n",
        "            \"Based on this, provide the 'final_score', 'improvements' (from identified flaws in the capability report, or state 'No significant changes yet' if applicable), and an 'overall_summary'.\"\n",
        "        )\n",
        "\n",
        "        # 3. Create a prompt template\n",
        "        prompt_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", human_message)\n",
        "        ])\n",
        "\n",
        "        # 4. Invoke the LLM with fallbacks, ensuring structured output\n",
        "        structured_llm = llm_with_fallbacks.with_structured_output(FinalReport)\n",
        "\n",
        "        # First apply the prompt template, then pipe to structured_llm\n",
        "        full_chain = prompt_template | structured_llm\n",
        "        final_report_pydantic = full_chain.invoke({\n",
        "            \"capability_report\": capability_report,\n",
        "            \"initial_audit_score\": initial_audit_score,\n",
        "            \"iteration\": current_iteration\n",
        "        })\n",
        "\n",
        "        final_report_pydantic.initial_score = initial_audit_score\n",
        "        final_report_pydantic.final_score = final_score\n",
        "\n",
        "        # 5. Convert the Pydantic object to a JSON string\n",
        "        state['final_report'] = final_report_pydantic.model_dump_json(indent=2)\n",
        "        print(\"✅ Final report synthesized successfully.\")\n",
        "        state['logs'] = append_log(state, \"synthesize_report\", \"SUCCESS\", {\"final_score\": final_score, \"iteration\": current_iteration})\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error during final report synthesis: {e}\"\n",
        "        state['error_message'] = error_msg\n",
        "        print(state['error_message'])\n",
        "        state['logs'] = append_log(state, \"synthesize_report\", \"ERROR\", {\"error\": str(e), \"iteration\": current_iteration})\n",
        "\n",
        "    return state\n",
        "\n",
        "print(\"synthesize_report function (LangGraph node) is defined, along with FinalReport Pydantic model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrEc_zbywjC5"
      },
      "source": [
        "<h1>4. Defining the Graph Logic</h1>\n",
        "\n",
        "<h3>4.1. Conditional Edge: Routing for Correction</h3>\n",
        "\n",
        "This function, `route_for_correction`, defines the conditional logic that enables our workflow to loop. It acts as a router after the `audit_capabilities` node has run.\n",
        "\n",
        "1.  It reads the `capability_report` and the current `iteration` count from the state.\n",
        "2.  It parses the `Trust Score` from the report.\n",
        "3.  **If** the score is below 95 and the number of correction loops is less than `MAX_ITERATIONS`, it returns the string `\"generate_corrections\"`. This tells LangGraph to proceed to the correction node.\n",
        "4.  **Else** (if the score is high enough or we've iterated too many times), it returns `\"synthesize_report\"`, directing the graph to the final reporting step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de3c0fde",
        "outputId": "56c7679a-fd68-4fc9-e7cb-07a44ba6cab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "route_for_correction function (LangGraph conditional edge) is defined.\n"
          ]
        }
      ],
      "source": [
        "# Define a constant for maximum iterations\n",
        "MAX_ITERATIONS = 3 # Or 5, depending on desired robustness\n",
        "\n",
        "def route_for_correction(state: GraphState) -> str:\n",
        "    \"\"\"\n",
        "    Conditional edge function to determine the next step in the graph.\n",
        "    Routes to 'generate_corrections' if score is low and max iterations not met,\n",
        "    otherwise routes to 'synthesize_report'.\n",
        "    Also sets the initial_audit_score if it's the first run.\n",
        "    \"\"\"\n",
        "    print(\"---ROUTING FOR CORRECTION---\")\n",
        "\n",
        "    capability_report = state.get('capability_report')\n",
        "    initial_audit_score = state.get('initial_audit_score')\n",
        "    iteration = state.get('iteration', 0) # Default to 0 if not set\n",
        "\n",
        "    if not capability_report:\n",
        "        print(\"No capability report found. Routing to synthesize_report as a fallback.\")\n",
        "        return \"synthesize_report\"\n",
        "\n",
        "    # Parse the Trust Score from the capability_report\n",
        "    trust_score_match = re.search(r'Trust Score: (\\d+)', capability_report)\n",
        "    if not trust_score_match:\n",
        "        print(\"Trust Score not found in capability report. Routing to synthesize_report as a fallback.\")\n",
        "        return \"synthesize_report\"\n",
        "\n",
        "    current_trust_score = int(trust_score_match.group(1))\n",
        "    print(f\"Current Trust Score: {current_trust_score}, Iteration: {iteration}/{MAX_ITERATIONS}\")\n",
        "\n",
        "    # Set initial_audit_score if it's the first run (iteration 0)\n",
        "    if initial_audit_score is None:\n",
        "        state['initial_audit_score'] = current_trust_score\n",
        "        print(f\"Initial Audit Score set to: {current_trust_score}\")\n",
        "\n",
        "    # Conditional logic\n",
        "    if current_trust_score < 95 and iteration < MAX_ITERATIONS:\n",
        "        print(\"Score below 95 and max iterations not reached. Routing to generate_corrections.\")\n",
        "        return \"generate_corrections\"\n",
        "    else:\n",
        "        print(\"Score >= 95 OR max iterations reached. Routing to synthesize_report.\")\n",
        "        return \"synthesize_report\"\n",
        "\n",
        "print(\"route_for_correction function (LangGraph conditional edge) is defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwd2gGP4wjC6"
      },
      "source": [
        "<h3>4.2. Assembling and Compiling the Workflow</h3>\n",
        "\n",
        "Here, we assemble all the components into a complete, runnable workflow using `StateGraph`.\n",
        "\n",
        "1.  **Initialize Graph**: A `StateGraph` is created with our `GraphState` as its schema.\n",
        "2.  **Add Nodes**: Each of our functions (`ingest_notebook`, `analyze_code`, etc.) is added as a node in the graph.\n",
        "3.  **Set Entry Point**: We specify that the workflow must always start at the `ingest_notebook` node.\n",
        "4.  **Add Edges**: We define the connections between the nodes. This creates the primary path: `ingest` -> `analyze_code` -> `analyze_documentation` -> `audit_capabilities`.\n",
        "5.  **Add Conditional Edge**: We use the `route_for_correction` function to create a branching point after the audit. The graph will either loop back via `generate_corrections` or proceed to `synthesize_report`.\n",
        "6.  **Compile**: The graph definition is compiled into a runnable application, `app`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f2ccaca",
        "outputId": "3a8ba588-ae4a-49a8-9824-c51a926b1cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StateGraph workflow defined, nodes added, and edges connected.\n"
          ]
        }
      ],
      "source": [
        "# 1. Initialize StateGraph with GraphState as its schema\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# 2. Add each of the six functions as nodes\n",
        "workflow.add_node(\"ingest_notebook\", ingest_notebook)\n",
        "workflow.add_node(\"analyze_code\", analyze_code)\n",
        "workflow.add_node(\"analyze_documentation\", analyze_documentation)\n",
        "workflow.add_node(\"audit_capabilities\", audit_capabilities)\n",
        "workflow.add_node(\"generate_corrections\", generate_corrections)\n",
        "workflow.add_node(\"synthesize_report\", synthesize_report)\n",
        "\n",
        "# 3. Set the entry point\n",
        "workflow.set_entry_point(\"ingest_notebook\")\n",
        "\n",
        "# 4. Define linear edges\n",
        "workflow.add_edge(\"ingest_notebook\", \"analyze_code\")\n",
        "workflow.add_edge(\"analyze_code\", \"analyze_documentation\")\n",
        "workflow.add_edge(\"analyze_documentation\", \"audit_capabilities\")\n",
        "\n",
        "# 5. Define the conditional edge after audit_capabilities\n",
        "workflow.add_conditional_edges(\n",
        "    \"audit_capabilities\", # Source node\n",
        "    route_for_correction, # Conditional function\n",
        "    {\n",
        "        \"generate_corrections\": \"generate_corrections\", # If route_for_correction returns \"generate_corrections\"\n",
        "        \"synthesize_report\": \"synthesize_report\"      # If route_for_correction returns \"synthesize_report\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# 6. Define the loop edge from generate_corrections back to analyze_code\n",
        "workflow.add_edge(\"generate_corrections\", \"analyze_code\")\n",
        "\n",
        "# 7. Define the end edge from synthesize_report to END\n",
        "workflow.add_edge(\"synthesize_report\", END)\n",
        "\n",
        "# Compile the workflow\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"StateGraph workflow defined, nodes added, and edges connected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJraYgW_wjC6"
      },
      "source": [
        "<h1>5. Execution and Output</h1>\n",
        "\n",
        "<h3>5.1. Helper Functions: Saving Outputs</h3>\n",
        "\n",
        "This cell defines two helper functions designed to save the final outputs of our workflow.\n",
        "\n",
        "1.  **`save_corrected_notebook`**: Takes the final `GraphState`, which contains the corrected code and markdown, and reconstructs a valid `.ipynb` notebook file from that content. This provides a tangible, usable artifact from the audit and correction process.\n",
        "2.  **`save_execution_logs`**: Takes the final `GraphState` and saves the `logs` list to a JSON file. This creates a detailed audit trail of the entire process, which is useful for debugging and review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J_o4iD65jwnM"
      },
      "outputs": [],
      "source": [
        "def save_corrected_notebook(state, filename=\"corrected_notebook.ipynb\"):\n",
        "    \"\"\"\n",
        "    Reconstructs a valid .ipynb file from the agent's 'notebook_content' state,\n",
        "    preserving the original cell structure.\n",
        "    \"\"\"\n",
        "    content = state.get(\"notebook_content\")\n",
        "    if not content or not content.get('cells'):\n",
        "        print(\"❌ No structured notebook content ('cells' key) found to save.\")\n",
        "        return\n",
        "\n",
        "    cells_to_save = []\n",
        "\n",
        "    for original_cell in content['cells']:\n",
        "        cell_type = original_cell.get('cell_type')\n",
        "        source = original_cell.get('source', '')\n",
        "\n",
        "        # Ensure source is a list of strings, each ending with a newline, for .ipynb format\n",
        "        if isinstance(source, str):\n",
        "            source_lines = [line + \"\\n\" for line in source.split('\\n')]\n",
        "        elif isinstance(source, list):\n",
        "            # Ensure each line in the list ends with a newline, if not already\n",
        "            source_lines = [line if line.endswith('\\n') else line + '\\n' for line in source]\n",
        "        else:\n",
        "            source_lines = [str(source) + '\\n'] # Convert non-string source to string and add newline\n",
        "\n",
        "        if cell_type == 'markdown':\n",
        "            new_cell = {\n",
        "                \"cell_type\": \"markdown\",\n",
        "                \"metadata\": {},\n",
        "                \"source\": source_lines\n",
        "            }\n",
        "        elif cell_type == 'code':\n",
        "            new_cell = {\n",
        "                \"cell_type\": \"code\",\n",
        "                \"execution_count\": None, # Set to None for a newly saved notebook\n",
        "                \"metadata\": {},\n",
        "                \"outputs\": [],\n",
        "                \"source\": source_lines\n",
        "            }\n",
        "        else:\n",
        "            # Handle unknown cell types or skip them\n",
        "            print(f\"⚠️ Warning: Unknown cell type '{cell_type}' encountered. Skipping cell.\")\n",
        "            continue\n",
        "        cells_to_save.append(new_cell)\n",
        "\n",
        "    # 3. Assemble the Notebook Structure\n",
        "    notebook_json = {\n",
        "        \"cells\": cells_to_save,\n",
        "        \"metadata\": {\n",
        "            \"kernelspec\": {\n",
        "                \"display_name\": \"Python 3\",\n",
        "                \"language\": \"python\",\n",
        "                \"name\": \"python3\"\n",
        "            },\n",
        "            \"language_info\": {\n",
        "                \"codemirror_mode\": {\"name\": \"ipython\", \"version\": 3},\n",
        "                \"file_extension\": \".py\",\n",
        "                \"mimetype\": \"text/x-python\",\n",
        "                \"name\": \"python\",\n",
        "                \"nbconvert_exporter\": \"python\",\n",
        "                \"pygments_lexer\": \"ipython3\",\n",
        "                \"version\": \"3.10.12\" # Use a default or try to extract from original if available\n",
        "            }\n",
        "        },\n",
        "        \"nbformat\": 4,\n",
        "        \"nbformat_minor\": 5\n",
        "    }\n",
        "\n",
        "    # 4. Write to Disk\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(notebook_json, f, indent=2)\n",
        "        print(f\"✅ SUCCESS: Corrected notebook saved to '{filename}'\")\n",
        "        print(f\"⬇️  Check the 'Files' sidebar in Colab to download it!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: Failed to save corrected notebook to '{filename}': {e}\")\n",
        "\n",
        "def save_execution_logs(state, filename=\"execution_logs.json\"):\n",
        "    \"\"\"\n",
        "    Saves the accumulated logs from the GraphState to a JSON file.\n",
        "    \"\"\"\n",
        "    logs = state.get('logs', [])\n",
        "\n",
        "    if not logs:\n",
        "        print(\"ℹ️ No logs found to save.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(logs, f, indent=2)\n",
        "        print(f\"✅ SUCCESS: Execution logs saved to '{filename}'\")\n",
        "        print(f\"⬇️  Check the 'Files' sidebar in Colab to download it!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: Failed to save execution logs to '{filename}': {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JjqyvvcwjC7"
      },
      "source": [
        "<h3>5.2. Running the Auto-Auditor Workflow</h3>\n",
        "\n",
        "This is the main execution block where we run the entire Auto-Auditor workflow.\n",
        "\n",
        "1.  **Configuration**: We set a `recursion_limit` to allow for the looping behavior of the graph.\n",
        "2.  **Initial State**: We define the starting state for the graph, providing the `target_url` of the Kaggle notebook we want to audit. The user is prompted to enter a URL, with a default provided as a fallback. The `iteration` count and `logs` list are also initialized.\n",
        "3.  **Execution**: We call `app.stream()` to run the workflow. The `stream` method allows us to see the output from each node as it completes, providing a real-time log of the agent's progress. The final state is captured as the stream progresses.\n",
        "4.  **Final Output**: Once the stream is complete, the final state is passed to the `save_corrected_notebook` and `save_execution_logs` functions, which write the final, improved notebook and the process logs to files. The final JSON report is also printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kszXforMhGdw",
        "outputId": "5153468a-097b-4d56-aae6-b1f8861efdf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Kaggle URL (or press Enter for default: https://www.kaggle.com/code/jhoward/jupyter-notebook-101): https://www.kaggle.com/code/colinmorris/strings-and-dictionaries\n",
            "Using user-provided URL: https://www.kaggle.com/code/colinmorris/strings-and-dictionaries\n",
            "🚀 LAUNCHING KAGGLE AUTO-AUDITOR (Full Loop Mode)...\n",
            "\n",
            "--- Workflow Events ---\n",
            "---INGESTING KAGGLE NOTEBOOK---\n",
            "KaggleApi client initialized and authenticated.\n",
            "Parsed URL: Owner='colinmorris', Kernel='strings-and-dictionaries'\n",
            "Executing Kaggle CLI command: kaggle kernels pull colinmorris/strings-and-dictionaries -p /tmp/tmpgfiwobm9/kaggle_notebook_colinmorris_strings-and-dictionaries\n",
            "Kaggle CLI stdout: Source code downloaded to /tmp/tmpgfiwobm9/kaggle_notebook_colinmorris_strings-and-dictionaries/strings-and-dictionaries.ipynb\n",
            "\n",
            "Found .ipynb file: /tmp/tmpgfiwobm9/kaggle_notebook_colinmorris_strings-and-dictionaries/strings-and-dictionaries.ipynb\n",
            "Copied .ipynb to persistent temp file: /tmp/tmpx23d1w3n.ipynb\n",
            "Successfully extracted content from strings-and-dictionaries.\n",
            "Cleaned up temporary file: /tmp/tmpx23d1w3n.ipynb\n",
            "🟢 Finished Step: ingest_notebook\n",
            "---ANALYZING CODE--- \n",
            "✅ Code analysis report generated successfully.\n",
            "🟢 Finished Step: analyze_code\n",
            "---ANALYZING DOCUMENTATION---\n",
            "✅ Documentation analysis report generated successfully.\n",
            "🟢 Finished Step: analyze_documentation\n",
            "---AUDITING CAPABILITIES---\n",
            "✅ Capability audit report generated successfully.\n",
            "Initial Audit Score set to: 93\n",
            "---ROUTING FOR CORRECTION---\n",
            "Current Trust Score: 93, Iteration: 0/3\n",
            "Score below 95 and max iterations not reached. Routing to generate_corrections.\n",
            "🟢 Finished Step: audit_capabilities\n",
            "   🔍 Audit Complete. Current Trust Score: 93\n",
            "--GENERATING CORRECTIONS--\n",
            "✅ Corrections generated successfully. Iteration: 1\n",
            "🟢 Finished Step: generate_corrections\n",
            "   ✨ CORRECTIONS GENERATED! Looping back to analysis...\n",
            "---ANALYZING CODE--- \n",
            "✅ Code analysis report generated successfully.\n",
            "🟢 Finished Step: analyze_code\n",
            "---ANALYZING DOCUMENTATION---\n",
            "✅ Documentation analysis report generated successfully.\n",
            "🟢 Finished Step: analyze_documentation\n",
            "---AUDITING CAPABILITIES---\n",
            "✅ Capability audit report generated successfully.\n",
            "---ROUTING FOR CORRECTION---\n",
            "Current Trust Score: 70, Iteration: 1/3\n",
            "Score below 95 and max iterations not reached. Routing to generate_corrections.\n",
            "🟢 Finished Step: audit_capabilities\n",
            "   🔍 Audit Complete. Current Trust Score: 70\n",
            "--GENERATING CORRECTIONS--\n",
            "✅ Corrections generated successfully. Iteration: 2\n",
            "🟢 Finished Step: generate_corrections\n",
            "   ✨ CORRECTIONS GENERATED! Looping back to analysis...\n",
            "---ANALYZING CODE--- \n",
            "✅ Code analysis report generated successfully.\n",
            "🟢 Finished Step: analyze_code\n",
            "---ANALYZING DOCUMENTATION---\n",
            "✅ Documentation analysis report generated successfully.\n",
            "🟢 Finished Step: analyze_documentation\n",
            "---AUDITING CAPABILITIES---\n",
            "✅ Capability audit report generated successfully.\n",
            "---ROUTING FOR CORRECTION---\n",
            "Current Trust Score: 97, Iteration: 2/3\n",
            "Score >= 95 OR max iterations reached. Routing to synthesize_report.\n",
            "🟢 Finished Step: audit_capabilities\n",
            "   🔍 Audit Complete. Current Trust Score: 97\n",
            "---SYNTHESIZING FINAL REPORT---\n",
            "✅ Final report synthesized successfully.\n",
            "🟢 Finished Step: synthesize_report\n",
            "\n",
            "✅ WORKFLOW FINISHED SUCCESSFULLY\n",
            "✅ SUCCESS: Corrected notebook saved to 'corrected_notebook_v2.ipynb'\n",
            "⬇️  Check the 'Files' sidebar in Colab to download it!\n",
            "✅ SUCCESS: Execution logs saved to 'execution_logs.json'\n",
            "⬇️  Check the 'Files' sidebar in Colab to download it!\n",
            "\n",
            "--- Final Report ---\n",
            "{\n",
            "  \"initial_score\": 93,\n",
            "  \"final_score\": 97,\n",
            "  \"improvements\": [\n",
            "    \"Introduced function encapsulation and type hinting for improved modularity and reusability.\",\n",
            "    \"Implemented error handling examples, specifically for ValueError, to demonstrate robust programming.\",\n",
            "    \"Enhanced introductory motivation in documentation to better contextualize the importance of strings and dictionaries in data science.\"\n",
            "  ],\n",
            "  \"overall_summary\": \"This Kaggle notebook is an exceptionally well-crafted and highly effective educational resource for fundamental Python string and dictionary operations. It demonstrates outstanding clarity, correctness, and comprehensive coverage of essential concepts. While minor areas for enhancement were identified, primarily concerning the transition to production-grade code practices and a minor motivational aspect, these do not detract from its primary tutorial purpose. The audit process confirmed its high quality and educational value, with recommendations aimed at further robustness and broader applicability.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "config = RunnableConfig(recursion_limit=30)\n",
        "\n",
        "# 1. Implement User Input with Fallback Logic\n",
        "default_url = \"https://www.kaggle.com/code/jhoward/jupyter-notebook-101\"\n",
        "user_input_url = input(f\"Enter Kaggle URL (or press Enter for default: {default_url}): \")\n",
        "\n",
        "target_url = default_url\n",
        "if user_input_url:\n",
        "    if user_input_url.startswith(\"https://www.kaggle.com/code/\"):\n",
        "        target_url = user_input_url\n",
        "        print(f\"Using user-provided URL: {target_url}\")\n",
        "    else:\n",
        "        print(\"⚠️ Invalid Kaggle URL format. Falling back to default URL.\")\n",
        "        print(f\"Using default URL: {target_url}\")\n",
        "else:\n",
        "    print(f\"Using default URL: {target_url}\")\n",
        "\n",
        "initial_state = {\n",
        "    \"target_url\": target_url,\n",
        "    \"notebook_content\": None, \"code_report\": None, \"doc_report\": None,\n",
        "    \"capability_report\": None, \"final_report\": None, \"error_message\": None,\n",
        "    \"iteration\": 0, # Initialize iteration\n",
        "    \"logs\": [] # Initialize logs list\n",
        "}\n",
        "\n",
        "print(\"🚀 LAUNCHING KAGGLE AUTO-AUDITOR (Full Loop Mode)...\")\n",
        "\n",
        "final_state = None # Initialize final_state to be captured from stream\n",
        "\n",
        "try:\n",
        "    print(\"\\n--- Workflow Events ---\")\n",
        "    for event in app.stream(initial_state, config=config):\n",
        "        for key, value in event.items():\n",
        "            print(f\"🟢 Finished Step: {key}\")\n",
        "            # Update final_state with the latest full state from the event.\n",
        "            # The `value` in an event item is the state after that node executed.\n",
        "            # We update `initial_state` which acts as our accumulating state.\n",
        "            initial_state.update(value)\n",
        "\n",
        "            # Check for specific milestones\n",
        "            if key == \"generate_corrections\":\n",
        "                print(\"   ✨ CORRECTIONS GENERATED! Looping back to analysis...\")\n",
        "            elif key == \"audit_capabilities\":\n",
        "                # Safely access the score from the state update\n",
        "                report = value.get('capability_report', '')\n",
        "                score_match = re.search(r'Trust Score: (\\d+)', report)\n",
        "                current_score = int(score_match.group(1)) if score_match else 'N/A'\n",
        "                print(f\"   🔍 Audit Complete. Current Trust Score: {current_score}\")\n",
        "\n",
        "            # If the workflow reached END, the final state is in initial_state\n",
        "            if key == \"synthesize_report\": # This is the last node before END\n",
        "                final_state = initial_state.copy()\n",
        "                break # Break from inner loop once synthesize_report is hit\n",
        "\n",
        "        if final_state: # If synthesize_report was processed, we're done\n",
        "            break\n",
        "\n",
        "    print(\"\\n✅ WORKFLOW FINISHED SUCCESSFULLY\")\n",
        "\n",
        "    # --- EXPORT STEP ---\n",
        "    # 3. Error Handling: Only call save_corrected_notebook if a final_state was captured\n",
        "    if final_state:\n",
        "        save_corrected_notebook(final_state, filename=\"corrected_notebook_v2.ipynb\")\n",
        "        save_execution_logs(final_state, filename=\"execution_logs.json\") # Save execution logs\n",
        "        if final_state.get('final_report'):\n",
        "            print(\"\\n--- Final Report ---\")\n",
        "            print(final_state['final_report'])\n",
        "    else:\n",
        "        print(\"❌ Workflow completed, but no final state was captured for saving the notebook and logs.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ WORKFLOW FAILED: {e}\")\n",
        "    if initial_state.get('error_message'):\n",
        "        print(f\"  Error details from state: {initial_state['error_message']}\")\n",
        "    # Attempt to save logs even on failure\n",
        "    save_execution_logs(initial_state, filename=\"execution_logs_error.json\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}